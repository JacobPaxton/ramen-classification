{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "987981ee",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook explores the cleaned ramen-ratings CSV. Any engineered features or scripts will be added to explore.py.\n",
    "\n",
    "# Findings\n",
    "1. Ramen packaging and five-star ratings are independent.\n",
    "2. Ramen country of origin and five-star ratings **have a dependent relationship.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d27764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2021de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (1515, 5) Validate size: (506, 5) Test size: (506, 5)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1515 entries, 2566 to 2127\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   brand       1515 non-null   object\n",
      " 1   name        1515 non-null   object\n",
      " 2   package     1515 non-null   object\n",
      " 3   country     1515 non-null   object\n",
      " 4   five_stars  1515 non-null   bool  \n",
      "dtypes: bool(1), object(4)\n",
      "memory usage: 60.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# create train split for exploration\n",
    "train, _, _ = wrangle.prep_explore()\n",
    "print('')\n",
    "\n",
    "# check work\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc145762",
   "metadata": {},
   "source": [
    "# Initial Exploration of Ramen Packaging\n",
    "Let's check if there's a dependent relationship between packaging and our target.\n",
    "\n",
    "Hypotheses:\n",
    "- $H_0$: Packaging and five-star ratings are independent.\n",
    "- $H_a$: Packaging and five-star ratings have a dependent relationship.\n",
    "\n",
    "Confidence interval: 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd793f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set confidence interval\n",
    "alpha = .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121014e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dependence of packaging and target\n",
    "package_5star_crosstab = pd.crosstab(train.package, train.five_stars)\n",
    "_, p, _, _ = stats.chi2_contingency(package_5star_crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771e5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaging and five-star ratings are independent, did not pass 95% confidence interval.\n",
      "p-value: 0.43924606238117814\n"
     ]
    }
   ],
   "source": [
    "# check if p is significant\n",
    "if p < alpha:\n",
    "    print(\"Packaging and five-star ratings have a dependent relationship with 95% confidence.\")\n",
    "    print(\"p-value:\", p)\n",
    "else:\n",
    "    print(\"Packaging and five-star ratings are independent, did not pass 95% confidence interval.\")\n",
    "    print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e8e31",
   "metadata": {},
   "source": [
    "**Packaging and five-star ratings are independent.** We will not use 'package' in our predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021715d4",
   "metadata": {},
   "source": [
    "# Initial Exploration of Ramen Country of Origin\n",
    "Let's check if there's a dependent relationship between country and our target.\n",
    "\n",
    "Hypotheses:\n",
    "- $H_0$: Country of origin and five-star ratings are independent.\n",
    "- $H_a$: Country of origin and five-star ratings have a dependent relationship.\n",
    "\n",
    "Confidence interval: 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854971c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set confidence interval\n",
    "alpha = .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b738143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create crosstab for chi-square statistical test\n",
    "country_5star_crosstab = pd.crosstab(train.country, train.five_stars)\n",
    "# limit only to countries with sufficient value counts in crosstab (an assumption of chi-square)\n",
    "enough_values_mask = (country_5star_crosstab[False] > 5) & (country_5star_crosstab[True] > 5)\n",
    "# run chi-square test\n",
    "_, p, _, _ = stats.chi2_contingency(country_5star_crosstab[enough_values_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0983b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country of origin and five-star ratings have a dependent relationship with 95% confidence.\n",
      "p-value: 1.680501270556502e-06\n"
     ]
    }
   ],
   "source": [
    "# check if p is significant\n",
    "if p < alpha:\n",
    "    print(\"Country of origin and five-star ratings have a dependent relationship with 95% confidence.\")\n",
    "    print(\"p-value:\", p)\n",
    "else:\n",
    "    print(\"Country of origin and five-star ratings are independent, did not pass 95% confidence interval.\")\n",
    "    print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8864998",
   "metadata": {},
   "source": [
    "**Country of origin and five-star ratings have a dependent relationship.** We will further explore country of origin and consider using it in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c473b",
   "metadata": {},
   "source": [
    "# Initial Exploration of Ramen Brand\n",
    "Let's check if there's a dependent relationship between ramen brand and our target.\n",
    "\n",
    "Hypotheses:\n",
    "- $H_0$: Ramen brand and five-star ratings are independent.\n",
    "- $H_a$: Ramen brand and five-star ratings have a dependent relationship.\n",
    "\n",
    "Confidence interval: 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aababe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set confidence interval\n",
    "alpha = .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fe82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create crosstab for chi-square statistical test\n",
    "brand_5star_crosstab = pd.crosstab(train.brand, train.five_stars)\n",
    "# limit only to brands with sufficient value counts in crosstab (an assumption of chi-square)\n",
    "enough_values_mask = (brand_5star_crosstab[False] > 5) & (brand_5star_crosstab[True] > 5)\n",
    "# run chi-square test\n",
    "_, p, _, _ = stats.chi2_contingency(brand_5star_crosstab[enough_values_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d70b87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramen brand and five-star ratings are independent, did not pass 95% confidence interval.\n",
      "p-value: 0.22292206264805428\n"
     ]
    }
   ],
   "source": [
    "# check if p is significant\n",
    "if p < alpha:\n",
    "    print(\"Ramen brand and five-star ratings have a dependent relationship with 95% confidence.\")\n",
    "    print(\"p-value:\", p)\n",
    "else:\n",
    "    print(\"Ramen brand and five-star ratings are independent, did not pass 95% confidence interval.\")\n",
    "    print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11480ff7",
   "metadata": {},
   "source": [
    "**Ramen brand and five-star ratings are independent.** We will not use 'brand' in our predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e15c104",
   "metadata": {},
   "source": [
    "# Breaking Down the Ramen Product Name\n",
    "Nearly all ramen reviews in our dataset have a unique combination of brand and product name. Only 21 combinations of brand and product out of the nearly 1500 in our exploration split have two reviews, and there are no combinations with more than two reviews. Because of this low commonality, we can't run initial chi-square tests to see if product names have a dependent relationship with five star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c6877f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand+name with only one review: 1473\n",
      "Brand+name with two reviews: 21\n",
      "Brand+name with more than two reviews: 0\n"
     ]
    }
   ],
   "source": [
    "# show the review repeats and non-repeats\n",
    "print(\"Brand+name with only one review:\", (train[['brand','name']].value_counts() == 1).sum())\n",
    "print(\"Brand+name with two reviews:\", (train[['brand','name']].value_counts() == 2).sum())\n",
    "print(\"Brand+name with more than two reviews:\", (train[['brand','name']].value_counts() > 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604c51c",
   "metadata": {},
   "source": [
    "Before we can understand the relationship between product name and the target, we will need to split out certain keywords in product names to use as features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca03d4",
   "metadata": {},
   "source": [
    "## Identifying Keywords to Use\n",
    "**Ramen names will require multiple features** due to the fact that a ramen product can have many attributes.\n",
    "\n",
    "Another issue is that some product names use English and some do not use English. In order to accomodate this, **we will need to translate some words** to English and include them in our features (EX: put \"soy\" and \"shoyu\"/\"shouyu\" into one feature).\n",
    "\n",
    "### Word Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db6dab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['South Korea', 'USA', 'Hong Kong', 'UK', 'Thailand', 'Japan', 'Taiwan', 'Malaysia', 'India', 'Canada', 'Singapore', 'Philippines', 'China', 'Mexico', 'Indonesia', 'Cambodia', 'Netherlands', 'Australia', 'Nepal', 'Vietnam', 'Myanmar', 'Germany', 'Pakistan', 'Hungary', 'Colombia', 'Bangladesh', 'Brazil']\n"
     ]
    }
   ],
   "source": [
    "# print a list of countries with a ramen product\n",
    "print(train.country.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d386480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking product names by country (cell ran multiple times with different inputs)\n",
    "# train[train.country == 'Taiwan'].name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85eb97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking count of values matching the string (cell ran multiple times with different inputs)\n",
    "# (train.name.str.contains('Cake') == True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e7b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking row's values for rows containing matched string (cell ran multiple times with different inputs)\n",
    "# train[train.name.str.contains('Teriyaki')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7cf56",
   "metadata": {},
   "source": [
    "### Identifying Remaining Keywords\n",
    "The following keyword mask contains all the words I've designated as keywords. This keyword conglomeration will be unpacked in a readable format in the sections below this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5f658e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all keywords, prepare list for df.col.str.contains()\n",
    "keyword_mask = '|'.join(['Vermicelli', 'Vernicalli', 'Bihun', 'Sano', 'Chicken', 'Chikin', 'Duck',\n",
    "                         'Vegetable', 'Veggie', 'Vegetarian','Beef', 'Gomtang', 'Seolleongtang', 'Sukiyaki', \n",
    "                         'Nam Tok', 'Pork', 'Jjajangmen', 'Jiajang', 'Tonkotsu', 'Tomkotsu', 'Bacon', 'Budae',\n",
    "                         'Seafood', 'Crab', 'Anchovy', 'Bajirak', 'Clam', 'Abalone', 'Scallop', 'Vongole', \n",
    "                         'Salmon', 'Lobster', 'Shrimp', 'Prawn', 'Tuna', 'Tteok', 'Rabokki', 'Raobokki',\n",
    "                         'Spicy', 'Spice', 'Shin', 'Jjamppong', 'Jjambbong', 'Buldalk', 'Sutah', 'Budae', \n",
    "                         'Habanero', 'Jinjja', 'Jin', 'Yeul', 'Mala', 'Teumsae', 'Bibim', 'Picante', 'Bulnak', \n",
    "                         'Volcano', 'Odongtong', 'Sriracha', 'Arrabiata', 'Tom Yum', 'Tom Yam', 'Tom Saab', \n",
    "                         'Tom Klong', 'Suki', 'Stir Fry', 'Bokkeum', 'Tteokbokki', 'Topokki', 'Yukgaejang', \n",
    "                         'Rabokki', 'Yakisoba', 'Yaki-Soba', 'Yakiosoba', 'Fried', 'Goreng', 'Ramyonsari', \n",
    "                         'Keopnurungji', 'Sabalmyeon', 'Miso', 'Teriyaki', 'Mushroom', 'Udon', 'Udoin', \n",
    "                         'Tomato', 'Chili', 'Chilli', 'chili', 'Wonton', 'Wantan', 'Pickled', 'Sesame', \n",
    "                         'Superior', 'Carbonara', 'Chow Mein', 'Sweet', 'Pad Thai', 'Sour', 'sour', 'Curry', \n",
    "                         'Soy', 'Shoyu', 'Shiitake', 'Shitake', 'Tofu', 'Pho', 'Clear', 'Egg', 'Tempura', \n",
    "                         'Laksa', 'Buckwheat', 'Soba', 'Salt', 'Shio', 'Sio', 'Tomato', 'Neapolitan', \n",
    "                         'Napolitan', 'Spaghetti', 'Mayo', 'Barbecue', 'BBQ', 'Masala', 'Kimchi', 'Veg',\n",
    "                         'Tteobokki', 'Rice', 'Onion', 'Pollo', 'Cheese', 'Betawi', 'Chah Chiang',\n",
    "                         'Namja', 'Perisa', 'Kari', 'Jjawang', 'Jjajangmyeon', 'Sogokimyun', 'Jjajang',\n",
    "                         'Ossyoi', 'Befikr', 'curry', 'Sotanghon', 'U-Dong', 'U-dong', 'Mi Goreng', 'Kocok',\n",
    "                         'Chacharoni', 'Yakibuta', 'Cuchareable', 'RMy', 'Jalapeno', 'Biryani', 'Carne',\n",
    "                         'Kimchee', 'Pad Kee Mao', 'Kalguksoo', 'Prok', 'Nipis', 'Jjampong', 'Buldak',\n",
    "                         'tom Yum', 'Sesami', 'Kim Chee', 'Kebab', 'Hyoubanya', 'Batchoy', 'Gentong',\n",
    "                         'Kokomen', 'Requeijao', 'Champong', 'Gallina', 'Bulalo', 'Wasabi', 'Kalamansi',\n",
    "                         'Cabe', 'Oosterse', 'Kung Pao'])\n",
    "\n",
    "# create True/False for whether the row contains a keyword in the product name\n",
    "train['has_keyword'] = train.name.str.contains(keyword_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cda98a",
   "metadata": {},
   "source": [
    "Based on the above keywords, I ran the following two cells to check remaining values that I missed earlier. If I found a notable word, I added it to the above keyword list and re-ran the cells. I repeated this process until I was satisfied with the words I had designated as keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8597a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1377\n",
       "False     138\n",
       "Name: has_keyword, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if column is mostly True values\n",
    "train.has_keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06d56732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Noodles    34\n",
       "Noodle     30\n",
       "Ramen      19\n",
       "Instant    16\n",
       "Cup        14\n",
       "Flavour    11\n",
       "Sauce      11\n",
       "Flavor     10\n",
       "Rasa        9\n",
       "Mi          8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all rows without keywords for each unique word's value counts in entire list (ran this cell multiple times)\n",
    "(\n",
    "    pd.Series( # make a Series of each instance of each word\n",
    "        ' '.join(\n",
    "                 train[~train.has_keyword]    # look at rows we haven't caught with a keyword yet\n",
    "                 .name.tolist()        # put all 'name' cells in a list\n",
    "                ).split()        # join all lists into one string, then split the string into a list of each word\n",
    "    ).value_counts()        # calculate the value counts of each word in the series\n",
    "    .head(10)         # display the top 10 (changed from 30 to 10 after the words I wanted were captured)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c86df",
   "metadata": {},
   "source": [
    "### Grouping Keywords, Checking Value Counts\n",
    "Now that we have a keyword list, I will organize it into groups and check counts for each grouping. The goal of this is to prepare for group elimination of keywords in the next section. Each grouping will be considered as one feature or value; for example, the **'noodle_type'** feature would have a 'noodle' value that covers ['Noodle', 'Myeon', 'Myon']. \n",
    "\n",
    "#### Noodle Type\n",
    "* 'Noodle', 'Myeon', 'Myon' (665)\n",
    "* 'Udon', 'Udoin' (49)\n",
    "* 'Miso' (23)\n",
    "* 'Rice', 'Mi' (239)\n",
    "* 'Vermicelli', 'Vernicalli', 'Bihun', 'Sano' (43)\n",
    "* 'Rice Cake', 'Tteok', 'Rabokki', 'Raobokki' (4)\n",
    "* 'Wonton', 'Wantan' (5)\n",
    "* 'Spaghetti', 'Carbonara', 'Neapolitan', 'Napolitan' (10)\n",
    "* 'Buckwheat', 'Soba' (18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "121e6df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above noodle types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Buckwheat', 'Soba'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc8575",
   "metadata": {},
   "source": [
    "#### Meats\n",
    "* 'Chicken', 'Chikin', 'Duck', 'Pollo', 'Buldalk' (205)\n",
    "* 'Beef', 'Gomtang', 'Seolleongtang', 'Sukiyaki', 'Nam Tok', 'Sutah' (152)\n",
    "* 'Pork', 'Jjajangmen', 'Jiajang', 'Tonkotsu', 'Tomkotsu', 'Bacon', 'Budae' (108)\n",
    "* 'Seafood', 'Crab', 'Anchovy', 'Bajirak', 'Clam', 'Abalone', 'Scallop', 'Vongole', 'Salmon', 'Lobster', 'Shrimp', 'Prawn', 'Tuna', 'Jjamppong', 'Jjambbong' (198)\n",
    "* 'Chili', 'Chilli', 'chili' (35)\n",
    "* 'Chow Mein' (25)\n",
    "* 'Egg' (5)\n",
    "* 'Tofu' (2)\n",
    "* 'Barbecue', 'BBQ' (9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae13c89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above meat types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Barbecue', 'BBQ'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96dd208",
   "metadata": {},
   "source": [
    "#### Vegetables\n",
    "* 'Clear', 'Veg' (covers 'Vegetable', 'Veggie', 'Vegetarian' and 'Veg') (85)\n",
    "* 'Kimchi', 'Sabalmyeon' (22)\n",
    "* 'Mushroom', 'Shiitake', 'Shitake' (35)\n",
    "* 'Tomato' (21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c9173a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above veggie types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Tomato'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703636a",
   "metadata": {},
   "source": [
    "#### Taste\n",
    "* 'Spicy', 'Spice', 'Shin', 'Jjamppong'/'Jjambbong'(seafood), 'Buldalk'(chicken), 'Sutah'(beef), 'Budae'(sausage), 'Habanero', 'Jinjja', 'Jin', 'Yeul', 'Mala', 'Teumsae', 'Bibim', 'Picante', 'Bulnak', 'Volcano', 'Odongtong', 'Sriracha', 'Arrabiata', 'Tom Yum', 'Tom Yam', 'Tom Saab', 'Tom Klong', 'Suki', 'Laksa' (304)\n",
    "* 'Ramyonsari', 'Keopnurungji' (2)\n",
    "* 'Salt', 'Shio', 'Sio' (17)\n",
    "* 'Soy', 'Shoyu', 'Shouyu', 'Teriyaki' (70)\n",
    "* 'Mayo' (6)\n",
    "* 'Cheese' (11)\n",
    "* 'Sweet' (18)\n",
    "* 'Sour', 'sour' (19)\n",
    "* 'Curry' (68)\n",
    "* 'Sesame' (32)\n",
    "* 'Pickle' (11)\n",
    "* 'Masala' (9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0984e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above taste types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Masala'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b45498",
   "metadata": {},
   "source": [
    "#### Preparation\n",
    "* 'Instant', 'Minute', 'Ramyun', 'Jinjja', 'Bibim' (296)\n",
    "* 'Stir Fry', 'Bokkeum', 'Tteokbokki', 'Tteobokki', 'Topokki', 'Yukgaejang', 'Rabokki', 'Yakisoba', 'Yaki-Soba', 'Yakiosoba', 'Fried', 'Goreng', 'Tempura' (123)\n",
    "    * Eliminate 'Non-Fried' from this feature\n",
    "* 'Soup', 'Jjigae', 'Consomme' (109)\n",
    "* 'Pad Thai' (4)\n",
    "* 'Pho' (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64b84c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above preparation types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Pho'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795cb7d1",
   "metadata": {},
   "source": [
    "## Choosing Feasible Features\n",
    "Now that we have row counts for each grouping, we can begin to consider what features are viable. Here is what we should consider:\n",
    "1. A feature must have at least two values \n",
    "    ** EX: 'meat_type' feature has 'chicken', 'beef', 'pork', etc values\n",
    "1. A crosstab of the feature must have more than five values in each cell for the chi square statistical test\n",
    "1. The feature's values should be independent from one another \n",
    "    ** EX: 'taste_type' should not have individual 'sweet' and 'sour' values because some ramen have 'sweet & sour' in the product name\n",
    "    \n",
    "### Features that Pass the Above Requirements\n",
    "- noodle_type: \n",
    "    * **wheat** ('Udon', 'Udoin', 'U-Dong', 'U-dong', 'Sano', 'Spaghetti', 'Carbonara', 'Neapolitan', 'Napolitan', 'Kalguksoo') (63)\n",
    "    * **buckwheat** ('Buckwheat', 'Soba') (18)\n",
    "    * **rice** ('Rice', 'Vermicelli', 'Vernicalli', 'Bihun', 'Biryani') (106)\n",
    "- flavor: \n",
    "    * **miso** ('Miso') (23)\n",
    "    * **chicken** ('Chicken', 'Chikin', 'Duck', 'Pollo', 'Buldalk', 'Buldak', 'Requeijao', 'Gallina') (215)\n",
    "    * **beef** ('Beef', 'Gomtang', 'Seolleongtang', 'Sukiyaki', 'Nam Tok', 'Sutah', 'Sogokimyun', 'Cuchareable', 'Carne', 'Kebab', 'Gentong', 'Bulalo') (163)\n",
    "    * **pork** ('Pork', 'Prok', 'Jjajangmyeon', 'Jjajangmen', 'Jiajang', 'Jjajang', 'Chacharoni', 'Jjawang', 'Tonkotsu', 'Tomkotsu', 'Bacon', 'Ossyoi', 'Yakibuta', 'Batchoy') (115)\n",
    "    * **crustacean** ('Crab', 'Lobster', 'Shrimp', 'Prawn') (108)\n",
    "    * **mollusk** ('Bajirak', 'Clam', 'Abalone', 'Scallop', 'Vongole') (15)\n",
    "    * **chili** ('Chili', 'Chilli', 'chili', 'Cabe') (37)\n",
    "    * **curry** ('Curry', 'curry', 'Betawi', 'Perisa', 'Kari') (93)\n",
    "    * **chow_mein** ('Chow Mein') (25)\n",
    "    * **kimchi** ('Kimchi', 'Kimchee', 'Sabalmyeon', 'Kim Chee') (24)\n",
    "    * **mushroom** ('Mushroom', 'Shiitake', 'Shitake') (35)\n",
    "    * **tomato** ('Tomato') (21)\n",
    "    * **veggie** ('Clear', 'Veg', 'Oosterse') (86)\n",
    "    * **sesame** ('Sesame', 'Sesami') (33)\n",
    "    * **lime** ('Lime', 'Jeruk Nipis', 'Kalamansi') (11)\n",
    "- spicy:\n",
    "    * **True** ('Spicy', 'Spice', 'Shin', 'Jjamppong'/'Jjambbong'/'Jjampong'/'Champong'(seafood), 'Buldalk'(chicken), 'Sutah'(beef), 'Budae'(sausage), 'RMy', 'Habanero', 'Jinjja', 'Jin', 'Yeul', 'Mala', 'Teumsae', 'Bibim', 'Picante', 'Bulnak', 'Volcano', 'Odongtong', 'Sriracha', 'Arrabiata', 'Tom Yum', 'Tom Yam', 'tom Yum', 'Tom Saab', 'Tom Klong', 'Suki', 'Laksa', 'Chah Chiang', 'Namja', 'Befikr', 'Mi Goreng', 'Kocek', 'Jalapeno', 'Pad Kee Mao', 'Kokomen', 'Wasabi', 'Kung Pao') (360)\n",
    "    * **False** (Everything not listed)\n",
    "- fried:\n",
    "    * **True** ('Stir Fry', 'Bokkeum', 'Tteokbokki', 'Tteobokki', 'Topokki', 'Yukgaejang', 'Rabokki', 'Hyoubanya', 'Yakisoba', 'Yaki-Soba', 'Yakiosoba', 'Fried', 'Goreng', 'Tempura', 'Kung Pao') (127)\n",
    "    * **False** (Everything not listed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c49e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

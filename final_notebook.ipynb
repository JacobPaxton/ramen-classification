{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c1aead",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This project analyzes the Ramen Ratings dataset from Kaggle. This dataset has a few thousand ramen products and their ratings, from 0 to 5 stars. The data includes the review number, the ramen's brand, product name, packaging style, country of origin, rating, and whether or not the ramen is in the top 10. I chose to make this a one-vs-rest classification problem on whether a ramen is rated five stars or not. The analysis uses independence tests and feature engineering to arrive at key drivers of five-star ratings, then uses these features to build a predictive model. The project successfully identified key drivers using these methods and build a predictive model that performs better than a standard baseline for the work.\n",
    "\n",
    "## The cool new stuff I accomplished for this project\n",
    "- **Heavy keyword engineering**\n",
    "    * Domain research to understand ramen products based on keyword\n",
    "    * Translation to bring all keywords to consistent categories\n",
    "    * Categorization on common factors based on domain research and translation\n",
    "- **Multi-layered statistical testing to eliminate features**\n",
    "    * Chi-Square tests to eliminate initial features that are not related to target\n",
    "    * One-hot encoding of remaining features' categories\n",
    "    * Chi-Square tests to eliminate one-hot-encoded categories that are not related to target\n",
    "- **Clustering country and keyword features into low-, medium-, and high-rate five-star ratings groups**\n",
    "    * Checked proportions of five-star rating counts against not-five-star rating counts for True in encoded feature\n",
    "    * Checked proportions of five-star rating counts against not-five-star rating counts for False in encoded feature\n",
    "    * Compared five-star proportions to check increase/decrease in proportion from False to True\n",
    "    * Bracketed increasing, middle, and decreasing proportions from False to True\n",
    "    \n",
    "## Other stuff that I've done before\n",
    "- Wrangle\n",
    "    * Categorize and encode target into five_stars column (classes: is five-stars, isn't five stars)\n",
    "    * Fix some values, drop some nulls, outliers, and duplicate rows, get rid of unnecessary columns\n",
    "    * Create univariate visualizations\n",
    "- Explore\n",
    "    * Run Chi-Square testing to determine if feature is related to target\n",
    "    * Feature engineering (overall)\n",
    "    * Create bivariate visualizations\n",
    "    * Choose features for model\n",
    "- Model\n",
    "    * Choose optimization priorities for the model (F1 Score)\n",
    "    * Resample the target to address class imbalance\n",
    "    * Create baseline model and multiple algorithmic models with varying hyperparameter combinations\n",
    "    * Evaluate models on Validate (first out-of-sample split)\n",
    "    * Choose best model in terms of our optimization priority\n",
    "    * Calculate ROC AUC of baseline and best model\n",
    "    * Evaluate baseline and best model on Test split\n",
    "    \n",
    "## Findings\n",
    "1. The brand of ramen does not influence whether or not the ramen product has a five-star rating.\n",
    "1. The packaging of ramen does not influence whether or not the ramen product has a five-star rating.\n",
    "1. A ramen's country of origin has an influence on whether or not the ramen product has a five-star rating.\n",
    "    - Malaysia has the highest five-star rating proportion of all origin countries.\n",
    "    - Ramen originating from Malaysia, Singapore, or Taiwan have the highest proportion of five-star ratings.\n",
    "    - Ramen from Hong Kong, Japan, South Korea, or Indonesia have the next-highest proportions of five-star ratings.\n",
    "    - Ramen from China, Thailand, or USA have the lowest proportions of five-star ratings.\n",
    "    - China has the lowest five-star rating proportion of all origin countries.\n",
    "1. A ramen's noodle type does not influence whether or not the ramen product has a five-star rating.\n",
    "1. A ramen's flavor influences whether or not the ramen product has a five-star rating.\n",
    "    - Curry flavor has the highest proportion of five-star ratings for all flavor categories.\n",
    "    - Ramen with curry or sesame flavor have the highest proportions of five-star ratings.\n",
    "    - Ramen with pork flavor or the common crustaceans have the next-highest proportions of five-star ratings.\n",
    "    - Chicken- and beef- flavored ramen products have the lowest five-star rating proportions of all flavors.\n",
    "    - Chicken flavor has the lowest five-star rating proportions of all flavors.\n",
    "1. A ramen's spicy status influences whether or not the ramen product has a five-star rating.\n",
    "1. A ramen's fried status does not have an effect on whether or not the ramen product has a five-star rating.\n",
    "\n",
    "## Model Results\n",
    "- Features used: country and flavor brackets (as described above) and spicy status\n",
    "- Evaluation Metric: F1 Score\n",
    "- Best model: Logistic Regression\n",
    "- Model performance: outperforms the baseline on F1 Score and ROC AUC for unseen data\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a53d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "import wrangle\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ec9e9",
   "metadata": {},
   "source": [
    "# Wrangle\n",
    "## Bottom Line Up Front: What I Did for Wrangle\n",
    "1. Acquire ramen-ratings.csv from Kaggle\n",
    "1. Rename a United States value to USA\n",
    "1. Drop low-count ramen styles Box, Can, and Bar (8 rows)\n",
    "1. Drop countries with less than 5 cumulative observations (29 rows)\n",
    "1. Drop Unrated, nulls and duplicates (16 rows)\n",
    "1. Replace Stars column with five_stars column\n",
    "1. Drop 'Review #' and 'Top Ten' columns\n",
    "1. Rename columns for easier exploration\n",
    "1. Split cleaned data into Train, Validate, and Test splits for exploration and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46f2c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (1515, 5) Validate size: (506, 5) Test size: (506, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>name</th>\n",
       "      <th>package</th>\n",
       "      <th>country</th>\n",
       "      <th>five_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>Samyang</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Pack</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Nongshim</td>\n",
       "      <td>Shin Noodle Soup</td>\n",
       "      <td>Cup</td>\n",
       "      <td>USA</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>Doll</td>\n",
       "      <td>Hello Kitty Dim Sum Noodle Japanese Curry Flavour</td>\n",
       "      <td>Cup</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         brand                                               name package  \\\n",
       "2566   Samyang                                                Hot    Pack   \n",
       "332   Nongshim                                   Shin Noodle Soup     Cup   \n",
       "1363      Doll  Hello Kitty Dim Sum Noodle Japanese Curry Flavour     Cup   \n",
       "\n",
       "          country  five_stars  \n",
       "2566  South Korea       False  \n",
       "332           USA        True  \n",
       "1363    Hong Kong       False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrangle.py script to wrangle the data as described above\n",
    "train, _, _ = wrangle.prep_explore()\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37996e",
   "metadata": {},
   "source": [
    "# Explore\n",
    "## Bottom Line Up Front: What I Did for Explore\n",
    "- Statistical testing on Ramen Brands that found brand is independent of five-star outcomes\n",
    "- Statistical testing on Ramen Packaging that found packaging is independent of five-star outcomes\n",
    "- Statistical testing on Country of Origin that found country is related to five-star outcomes\n",
    "- Keyword engineering to categorize ramen products into noodle type, flavor, spicy status, and fried status categories\n",
    "- Statistical testing on new features that found noodle type and fried status have no impact on five-star outcomes\n",
    "- Statistical testing on new features that found ramen flavor and spicy status have an impact on five-star outcomes\n",
    "- Dropped specific countries and flavors that did not have at least 5 reviews with five-star rating\n",
    "- Analyzed proportions of five-star reviews to all reviews for each country and flavor category\n",
    "- Grouped into high-, medium-, and low-proportion brackets for country and for flavor category\n",
    "- Checked country and flavor category brackets along with spicy status in terms of five-star and non-five-star reviews\n",
    "- Chose these features for modeling\n",
    "\n",
    "## Ramen Brand is Independent\n",
    "Hypotheses:\n",
    "- $H_0$: Ramen brand and five-star ratings are independent.\n",
    "- $H_a$: Ramen brand and five-star ratings have a dependent relationship.\n",
    "\n",
    "Confidence interval: 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0f04cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramen brand and five-star ratings are independent, did not pass 95% confidence interval.\n",
      "p-value: 0.22292206264805428\n"
     ]
    }
   ],
   "source": [
    "# set confidence interval\n",
    "alpha = .05\n",
    "# create crosstab for chi-square statistical test\n",
    "brand_5star_crosstab = pd.crosstab(train.brand, train.five_stars)\n",
    "# limit only to brands with sufficient value counts in crosstab (an assumption of chi-square)\n",
    "enough_values_mask = (brand_5star_crosstab[False] > 5) & (brand_5star_crosstab[True] > 5)\n",
    "# run chi-square test\n",
    "_, p, _, _ = stats.chi2_contingency(brand_5star_crosstab[enough_values_mask])\n",
    "\n",
    "# check if p is significant\n",
    "if p < alpha:\n",
    "    print(\"Ramen brand and five-star ratings have a dependent relationship with 95% confidence.\")\n",
    "    print(\"p-value:\", p)\n",
    "else:\n",
    "    print(\"Ramen brand and five-star ratings are independent, did not pass 95% confidence interval.\")\n",
    "    print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf58fb",
   "metadata": {},
   "source": [
    "## Ramen Packaging is Independent\n",
    "Hypotheses:\n",
    "- $H_0$: Packaging and five-star ratings are independent.\n",
    "- $H_a$: Packaging and five-star ratings have a dependent relationship.\n",
    "\n",
    "Confidence interval: 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd62889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaging and five-star ratings are independent, did not pass 95% confidence interval.\n",
      "p-value: 0.43924606238117814\n"
     ]
    }
   ],
   "source": [
    "# set confidence interval\n",
    "alpha = .05\n",
    "# check dependence of packaging and target\n",
    "package_5star_crosstab = pd.crosstab(train.package, train.five_stars)\n",
    "_, p, _, _ = stats.chi2_contingency(package_5star_crosstab)\n",
    "# check if p is significant\n",
    "if p < alpha:\n",
    "    print(\"Packaging and five-star ratings have a dependent relationship with 95% confidence.\")\n",
    "    print(\"p-value:\", p)\n",
    "else:\n",
    "    print(\"Packaging and five-star ratings are independent, did not pass 95% confidence interval.\")\n",
    "    print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29df12",
   "metadata": {},
   "source": [
    "## Ramen Country of Origin is Related\n",
    "Hypotheses:\n",
    "- $H_0$: Country of origin and five-star ratings are independent.\n",
    "- $H_a$: Country of origin and five-star ratings have a dependent relationship.\n",
    "\n",
    "Confidence interval: 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f2bcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country of origin and five-star ratings have a dependent relationship with 95% confidence.\n",
      "p-value: 1.680501270556502e-06\n"
     ]
    }
   ],
   "source": [
    "# set confidence interval\n",
    "alpha = .05\n",
    "# create crosstab for chi-square statistical test\n",
    "country_5star_crosstab = pd.crosstab(train.country, train.five_stars)\n",
    "# limit only to countries with sufficient value counts in crosstab (an assumption of chi-square)\n",
    "enough_values_mask = (country_5star_crosstab[False] > 5) & (country_5star_crosstab[True] > 5)\n",
    "# run chi-square test\n",
    "_, p, _, _ = stats.chi2_contingency(country_5star_crosstab[enough_values_mask])\n",
    "# check if p is significant\n",
    "if p < alpha:\n",
    "    print(\"Country of origin and five-star ratings have a dependent relationship with 95% confidence.\")\n",
    "    print(\"p-value:\", p)\n",
    "else:\n",
    "    print(\"Country of origin and five-star ratings are independent, did not pass 95% confidence interval.\")\n",
    "    print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5057a",
   "metadata": {},
   "source": [
    "## Breaking Down the Ramen Product Name\n",
    "### Why We Need to Engineer Features for Product Name\n",
    "Nearly all ramen reviews in our dataset have a unique combination of brand and product name. Only 21 combinations of brand and product out of the nearly 1500 in our exploration split have two reviews, and there are no combinations with more than two reviews. Because of this low commonality, we can't run initial chi-square tests to see if product names have a dependent relationship with five star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d333b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand+name with only one review: 1473\n",
      "Brand+name with two reviews: 21\n",
      "Brand+name with more than two reviews: 0\n"
     ]
    }
   ],
   "source": [
    "# show the review repeats and non-repeats\n",
    "print(\"Brand+name with only one review:\", (train[['brand','name']].value_counts() == 1).sum())\n",
    "print(\"Brand+name with two reviews:\", (train[['brand','name']].value_counts() == 2).sum())\n",
    "print(\"Brand+name with more than two reviews:\", (train[['brand','name']].value_counts() > 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517dbc2",
   "metadata": {},
   "source": [
    "### Solving Uniqueness Through Categorizing Keywords\n",
    "The following keyword mask contains all the words I've designated as keywords. This keyword conglomeration will be unpacked in a readable format in the sections below this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0630d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all keywords, prepare list for df.col.str.contains()\n",
    "keyword_mask = '|'.join(['Vermicelli', 'Vernicalli', 'Bihun', 'Sano', 'Chicken', 'Chikin', 'Duck',\n",
    "                         'Vegetable', 'Veggie', 'Vegetarian','Beef', 'Gomtang', 'Seolleongtang', 'Sukiyaki', \n",
    "                         'Nam Tok', 'Pork', 'Jjajangmen', 'Jiajang', 'Tonkotsu', 'Tomkotsu', 'Bacon', 'Budae',\n",
    "                         'Seafood', 'Crab', 'Anchovy', 'Bajirak', 'Clam', 'Abalone', 'Scallop', 'Vongole', \n",
    "                         'Salmon', 'Lobster', 'Shrimp', 'Prawn', 'Tuna', 'Tteok', 'Rabokki', 'Raobokki',\n",
    "                         'Spicy', 'Spice', 'Shin', 'Jjamppong', 'Jjambbong', 'Buldalk', 'Sutah', 'Budae', \n",
    "                         'Habanero', 'Jinjja', 'Jin', 'Yeul', 'Mala', 'Teumsae', 'Bibim', 'Picante', 'Bulnak', \n",
    "                         'Volcano', 'Odongtong', 'Sriracha', 'Arrabiata', 'Tom Yum', 'Tom Yam', 'Tom Saab', \n",
    "                         'Tom Klong', 'Suki', 'Stir Fry', 'Bokkeum', 'Tteokbokki', 'Topokki', 'Yukgaejang', \n",
    "                         'Rabokki', 'Yakisoba', 'Yaki-Soba', 'Yakiosoba', 'Fried', 'Goreng', 'Ramyonsari', \n",
    "                         'Keopnurungji', 'Sabalmyeon', 'Miso', 'Teriyaki', 'Mushroom', 'Udon', 'Udoin', \n",
    "                         'Tomato', 'Chili', 'Chilli', 'chili', 'Wonton', 'Wantan', 'Pickled', 'Sesame', \n",
    "                         'Superior', 'Carbonara', 'Chow Mein', 'Sweet', 'Pad Thai', 'Sour', 'sour', 'Curry', \n",
    "                         'Soy', 'Shoyu', 'Shiitake', 'Shitake', 'Tofu', 'Pho', 'Clear', 'Egg', 'Tempura', \n",
    "                         'Laksa', 'Buckwheat', 'Soba', 'Salt', 'Shio', 'Sio', 'Tomato', 'Neapolitan', \n",
    "                         'Napolitan', 'Spaghetti', 'Mayo', 'Barbecue', 'BBQ', 'Masala', 'Kimchi', 'Veg',\n",
    "                         'Tteobokki', 'Rice', 'Onion', 'Pollo', 'Cheese', 'Betawi', 'Chah Chiang',\n",
    "                         'Namja', 'Perisa', 'Kari', 'Jjawang', 'Jjajangmyeon', 'Sogokimyun', 'Jjajang',\n",
    "                         'Ossyoi', 'Befikr', 'curry', 'Sotanghon', 'U-Dong', 'U-dong', 'Mi Goreng', 'Kocok',\n",
    "                         'Chacharoni', 'Yakibuta', 'Cuchareable', 'RMy', 'Jalapeno', 'Biryani', 'Carne',\n",
    "                         'Kimchee', 'Pad Kee Mao', 'Kalguksoo', 'Prok', 'Nipis', 'Jjampong', 'Buldak',\n",
    "                         'tom Yum', 'Sesami', 'Kim Chee', 'Kebab', 'Hyoubanya', 'Batchoy', 'Gentong',\n",
    "                         'Kokomen', 'Requeijao', 'Champong', 'Gallina', 'Bulalo', 'Wasabi', 'Kalamansi',\n",
    "                         'Cabe', 'Oosterse', 'Kung Pao'])\n",
    "\n",
    "# create True/False for whether the row contains a keyword in the product name\n",
    "train['has_keyword'] = train.name.str.contains(keyword_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b0dcf",
   "metadata": {},
   "source": [
    "Based on the above keywords, I ran the following two cells to check remaining values that I missed earlier. If I found a notable word, I added it to the above keyword list and re-ran the cells. I repeated this process until I was satisfied with the words I had designated as keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b321f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1377\n",
       "False     138\n",
       "Name: has_keyword, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if column is mostly True values\n",
    "train.has_keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f398e8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Noodles    34\n",
       "Noodle     30\n",
       "Ramen      19\n",
       "Instant    16\n",
       "Cup        14\n",
       "Flavour    11\n",
       "Sauce      11\n",
       "Flavor     10\n",
       "Rasa        9\n",
       "Mi          8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all rows without keywords for each unique word's value counts in entire list (ran this cell multiple times)\n",
    "(\n",
    "    pd.Series( # make a Series of each instance of each word\n",
    "        ' '.join(\n",
    "                 train[~train.has_keyword]    # look at rows we haven't caught with a keyword yet\n",
    "                 .name.tolist()        # put all 'name' cells in a list\n",
    "                ).split()        # join all lists into one string, then split the string into a list of each word\n",
    "    ).value_counts()        # calculate the value counts of each word in the series\n",
    "    .head(10)         # display the top 10 (changed from 30 to 10 after the words I wanted were captured)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850efe58",
   "metadata": {},
   "source": [
    "### Grouping Keywords, Checking Value Counts\n",
    "Now that we have a keyword list, I will organize it into groups and check counts for each grouping. The goal of this is to prepare for group elimination of keywords in the next section. Each grouping will be considered as one feature or value; for example, the **'noodle_type'** feature would have a 'noodle' value that covers ['Noodle', 'Myeon', 'Myon']. \n",
    "\n",
    "#### Noodle Type\n",
    "* 'Noodle', 'Myeon', 'Myon' (665)\n",
    "* 'Udon', 'Udoin' (49)\n",
    "* 'Miso' (23)\n",
    "* 'Rice', 'Mi' (239)\n",
    "* 'Vermicelli', 'Vernicalli', 'Bihun', 'Sano' (43)\n",
    "* 'Rice Cake', 'Tteok', 'Rabokki', 'Raobokki' (4)\n",
    "* 'Wonton', 'Wantan' (5)\n",
    "* 'Spaghetti', 'Carbonara', 'Neapolitan', 'Napolitan' (10)\n",
    "* 'Buckwheat', 'Soba' (18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "810bd0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above noodle types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Buckwheat', 'Soba'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2722db",
   "metadata": {},
   "source": [
    "#### Meats\n",
    "* 'Chicken', 'Chikin', 'Duck', 'Pollo', 'Buldalk' (205)\n",
    "* 'Beef', 'Gomtang', 'Seolleongtang', 'Sukiyaki', 'Nam Tok', 'Sutah' (152)\n",
    "* 'Pork', 'Jjajangmen', 'Jiajang', 'Tonkotsu', 'Tomkotsu', 'Bacon', 'Budae' (108)\n",
    "* 'Seafood', 'Crab', 'Anchovy', 'Bajirak', 'Clam', 'Abalone', 'Scallop', 'Vongole', 'Salmon', 'Lobster', 'Shrimp', 'Prawn', 'Tuna', 'Jjamppong', 'Jjambbong' (198)\n",
    "* 'Chili', 'Chilli', 'chili' (35)\n",
    "* 'Chow Mein' (25)\n",
    "* 'Egg' (5)\n",
    "* 'Tofu' (2)\n",
    "* 'Barbecue', 'BBQ' (9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de7ddb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above meat types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Barbecue', 'BBQ'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315adac",
   "metadata": {},
   "source": [
    "#### Vegetables\n",
    "* 'Clear', 'Veg' (covers 'Vegetable', 'Veggie', 'Vegetarian' and 'Veg') (85)\n",
    "* 'Kimchi', 'Sabalmyeon' (22)\n",
    "* 'Mushroom', 'Shiitake', 'Shitake' (35)\n",
    "* 'Tomato' (21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15791173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above veggie types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Tomato'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b169b92",
   "metadata": {},
   "source": [
    "#### Taste\n",
    "* 'Spicy', 'Spice', 'Shin', 'Jjamppong'/'Jjambbong'(seafood), 'Buldalk'(chicken), 'Sutah'(beef), 'Budae'(sausage), 'Habanero', 'Jinjja', 'Jin', 'Yeul', 'Mala', 'Teumsae', 'Bibim', 'Picante', 'Bulnak', 'Volcano', 'Odongtong', 'Sriracha', 'Arrabiata', 'Tom Yum', 'Tom Yam', 'Tom Saab', 'Tom Klong', 'Suki', 'Laksa' (304)\n",
    "* 'Ramyonsari', 'Keopnurungji' (2)\n",
    "* 'Salt', 'Shio', 'Sio' (17)\n",
    "* 'Soy', 'Shoyu', 'Shouyu', 'Teriyaki' (70)\n",
    "* 'Mayo' (6)\n",
    "* 'Cheese' (11)\n",
    "* 'Sweet' (18)\n",
    "* 'Sour', 'sour' (19)\n",
    "* 'Curry' (68)\n",
    "* 'Sesame' (32)\n",
    "* 'Pickle' (11)\n",
    "* 'Masala' (9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d623c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above taste types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Masala'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad670372",
   "metadata": {},
   "source": [
    "#### Preparation\n",
    "* 'Instant', 'Minute', 'Ramyun', 'Jinjja', 'Bibim' (296)\n",
    "* 'Stir Fry', 'Bokkeum', 'Tteokbokki', 'Tteobokki', 'Topokki', 'Yukgaejang', 'Rabokki', 'Yakisoba', 'Yaki-Soba', 'Yakiosoba', 'Fried', 'Goreng', 'Tempura' (123)\n",
    "* 'Soup', 'Jjigae', 'Consomme' (109)\n",
    "* 'Pad Thai' (4)\n",
    "* 'Pho' (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04e4b708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking row count having the above preparation types (ran this cell multiple times)\n",
    "train.name.str.contains(\"|\".join(['Lime', 'Jeruk Nipis', 'Kalamansi'])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6bfd9",
   "metadata": {},
   "source": [
    "### Choosing Feasible Features\n",
    "Now that we have row counts for each grouping, we can begin to consider what features are viable. Here is what we should consider:\n",
    "1. A feature must have at least two values \n",
    "    ** EX: 'meat_type' feature has 'chicken', 'beef', 'pork', etc values\n",
    "1. A crosstab of the feature must have more than five values in each cell for the chi square statistical test\n",
    "1. The feature's values should be independent from one another \n",
    "    ** EX: 'taste_type' should not have individual 'sweet' and 'sour' values because some ramen have 'sweet & sour' in the product name\n",
    "    \n",
    "#### Features that Pass the Above Requirements\n",
    "- noodle_type: \n",
    "    * **wheat** ('Udon', 'Udoin', 'U-Dong', 'U-dong', 'Sano', 'Spaghetti', 'Carbonara', 'Neapolitan', 'Napolitan', 'Kalguksoo') (63)\n",
    "    * **buckwheat** ('Buckwheat', 'Soba') (18)\n",
    "    * **rice** ('Rice', 'Vermicelli', 'Vernicalli', 'Bihun', 'Biryani', 'Tteokbokki', 'Tteobokki', 'Topokki', 'Rabokki') (109)\n",
    "- flavor: \n",
    "    * **miso** ('Miso') (23)\n",
    "    * **chicken** ('Chicken', 'Chikin', 'Duck', 'Pollo', 'Buldalk', 'Buldak', 'Requeijao', 'Gallina') (215)\n",
    "    * **beef** ('Beef', 'Gomtang', 'Seolleongtang', 'Sukiyaki', 'Nam Tok', 'Sutah', 'Sogokimyun', 'Cuchareable', 'Carne', 'Kebab', 'Gentong', 'Bulalo', 'Yukgaejang') (163)\n",
    "    * **pork** ('Pork', 'Prok', 'Jjajangmyeon', 'Jjajangmen', 'Jiajang', 'Jjajang', 'Chacharoni', 'Jjawang', 'Tonkotsu', 'Tomkotsu', 'Bacon', 'Ossyoi', 'Yakibuta', 'Batchoy') (115)\n",
    "    * **crustacean** ('Crab', 'Lobster', 'Shrimp', 'Prawn') (108)\n",
    "    * **mollusk** ('Bajirak', 'Clam', 'Abalone', 'Scallop', 'Vongole') (15)\n",
    "    * **chili** ('Chili', 'Chilli', 'chili', 'Cabe') (37)\n",
    "    * **curry** ('Curry', 'curry', 'Betawi', 'Perisa', 'Kari') (93)\n",
    "    * **chow_mein** ('Chow Mein') (25)\n",
    "    * **kimchi** ('Kimchi', 'Kimchee', 'Sabalmyeon', 'Kim Chee') (24)\n",
    "    * **mushroom** ('Mushroom', 'Shiitake', 'Shitake') (35)\n",
    "    * **tomato** ('Tomato') (21)\n",
    "    * **veggie** ('Clear', 'Veg', 'Oosterse') (86)\n",
    "    * **sesame** ('Sesame', 'Sesami') (33)\n",
    "    * **lime** ('Lime', 'Jeruk Nipis', 'Kalamansi') (11)\n",
    "- spicy:\n",
    "    * **True** ('Spicy', 'Spice', 'Shin', 'Jjamppong'/'Jjambbong'/'Jjampong'/'Champong'(seafood), 'Buldalk'/'Buldak'(chicken), 'Sutah'(beef), 'Budae'(sausage), 'RMy', 'Habanero', 'Jinjja', 'Jin', 'Yeul', 'Mala', 'Teumsae', 'Bibim', 'Picante', 'Bulnak', 'Volcano', 'Odongtong', 'Sriracha', 'Arrabiata', 'Tom Yum', 'Tom Yam', 'tom Yum', 'Tom Saab', 'Tom Klong', 'Suki', 'Laksa', 'Chah Chiang', 'Namja', 'Befikr', 'Mi Goreng', 'Kocek', 'Jalapeno', 'Pad Kee Mao', 'Kokomen', 'Wasabi', 'Kung Pao', 'Kimchi', 'Kimchee', 'Sabalmyeon', 'Kim Chee', 'Nam Tok', 'Sogokimyun', 'Gentong', 'Chili', 'Chilli', 'chili', 'Cabe', 'Yukgaejang', 'Yakisoba', 'Yaki-Soba', 'Yakiosoba') (446)\n",
    "    * **False** ('Miso', 'Requeijao', 'Seolleongtang', 'Sukiyaki', 'Jjajangmyeon', 'Jjajangmen', 'Jiajang', 'Jjajang', 'Chacharoni', 'Jjawang', 'Ossyoi', 'Batchoy', 'Bajirak', 'Mushroom', 'Shiitake', 'Shitake', 'Tomato', 'Clear') (99)\n",
    "- fried:\n",
    "    * **True** ('Stir Fry', 'Bokkeum', 'Tteokbokki', 'Tteobokki', 'Topokki', 'Yukgaejang', 'Rabokki', 'Yakisoba', 'Yaki-Soba', 'Yakiosoba', 'Fried', 'Goreng', 'Tempura', 'Kung Pao', 'Sukiyaki', 'Kebab', 'Gentong', 'Bulalo', 'Jjajangmyeon', 'Jjajangmen', 'Jiajang', 'Jjajang', 'Chacharoni', 'Jjawang', 'Tonkotsu', 'Tomkotsu', 'Bacon', 'Yakibuta', 'Batchoy', 'Chow Mein') (207)\n",
    "    * **False** ('Requeijao', 'Yakisoba', 'Yaki-Soba', 'Yakiosoba', 'Gomtang', 'Seolleongtang', 'Nam Tok', 'Sutah', 'Sogokimyun', 'Cuchareable', 'Gomtang', 'Yukgaejang', 'Ossyoi', 'Clear') (43)\n",
    "    \n",
    "### Creating the Features\n",
    "#### noodle_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "665155e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build script to create noodle_type feature, run here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638eab7f",
   "metadata": {},
   "source": [
    "#### flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbd8c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build script to create flavor feature, run here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e41845",
   "metadata": {},
   "source": [
    "#### spicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a52cb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build script to create spicy feature, run here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb4aed",
   "metadata": {},
   "source": [
    "#### fried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45b2c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build script to create fried feature, run here (eliminate non-fried)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72de23",
   "metadata": {},
   "source": [
    "### Testing New Features for Relationship to Target\n",
    "#### Feature noodle_type is Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a3d4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test noodle_type here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea63a3d",
   "metadata": {},
   "source": [
    "#### Feature flavor is Related to Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22c7f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test flavor here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d522162",
   "metadata": {},
   "source": [
    "#### Feature spicy is Related to Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d858b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test spicy here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d81157",
   "metadata": {},
   "source": [
    "#### Feature fried is Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1896c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test fried here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b810e656",
   "metadata": {},
   "source": [
    "### Results of Breaking Down Product Name\n",
    "- Features created:\n",
    "- Features eliminated:\n",
    "- Features kept:\n",
    "\n",
    "# Univatiate Look at Our Candidate Features\n",
    "- Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef777fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histograms here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a55d27",
   "metadata": {},
   "source": [
    "# Each Country and Flavor Into Brackets\n",
    "## Country\n",
    "### Check Total Reviews and Number of Five-Star Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d6d3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc983472",
   "metadata": {},
   "source": [
    "### Bracket Countries Based on Proportion of Five-Star Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "723ae4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bracket countries into high-, medium-, and low-proportion five star review brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9a8fa",
   "metadata": {},
   "source": [
    "## Flavor\n",
    "### Check Total Reviews and Number of Five-Star Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6a8e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f175b94",
   "metadata": {},
   "source": [
    "### Bracket Flavors Based on Proportion of Five-Star Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0beeae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bracket countries into high-, medium-, and low-proportion five star review brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a9a53",
   "metadata": {},
   "source": [
    "## Final Features: Bivariate Look in Terms of Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2e0ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize target/non-target charts for each feature here\n",
    "# include exact numbers and proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15ee98",
   "metadata": {},
   "source": [
    "## Results of Exploration\n",
    "- Features kept for modeling:\n",
    "\n",
    "# Model\n",
    "## Bottom Line Up Front: What I Did for Model\n",
    "- Prepared entire dataset with model features\n",
    "- Chose F1 Score as our main evaluation metric due to prioritizing accuracy in presence of imbalanced classes\n",
    "- Split dataset into Train, Validate, and Test\n",
    "- Applied SMOTE+Tomek resampling to fix the class imbalance in our target for the Train split\n",
    "- Built, fit several classification models and hyperparameter combinations on resampled Train split\n",
    "- Evaluated baseline and all model performances on Validate, chose best model (Logistic Regression)\n",
    "- Chose not to use Grid Search to optimize hyperparameters due to nature of Logistic Regression hyperparameters\n",
    "- Evaluated baseline and best model's ROC Curve AUC\n",
    "- Evaluated baseline and best model on sequestered Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ca2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10db6f4f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Using nothing more than analyzing ramen product names, I was able to build several categorical features that took into account domain knowledge and translation. Some of these keyword-engineering features were statistically related to our target and used in the model. In the end, our predictive model outperformed the baseline on common evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
